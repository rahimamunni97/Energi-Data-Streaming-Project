# âš¡ EnergiFlow: Priceâ€“Production Analytics Platform

**EnergiFlow** is a real-time data streaming and analytics platform for electricity **priceâ€“production analysis**, built using modern **Big Data technologies**.  
The system ingests live energy market data from public APIs, processes it via **Kafka**, stores it in **PostgreSQL**, and visualizes insights through an interactive **Streamlit dashboard**.

This project demonstrates an **end-to-end Big Data pipeline**, including streaming ingestion, data validation, aggregation, visualization, and decision support.

---

## ğŸ“Œ Project Objectives

- Stream electricity **spot prices** and **production data** in real time  
- Handle **multiple asynchronous Kafka data streams**  
- Store validated records in a relational database  
- Perform **daily aggregation and analytical joins**  
- Visualize trends and support **data-driven decisions**  
- Demonstrate real-world **Big Data system design**

---

## ğŸ—ï¸ System Architecture

```EnergiDataService APIs
â”‚
â–¼
Kafka Producer (Python)
â”‚
â–¼
Apache Kafka
â”‚
â–¼
Kafka Consumer (Python)
â”‚
â–¼
PostgreSQL (energi_records)
â”‚
â–¼
Streamlit Analytics Dashboard```


---

## ğŸ”„ Data Sources

The platform consumes data from the **Energi Data Service (Denmark)**.

### 1ï¸âƒ£ DeclarationProduction
- Electricity production by source (wind, gas, biomass, etc.)
- Metric: `Production (MWh)`
- Kafka topic: `declaration_topic`

### 2ï¸âƒ£ ElspotPrices
- Electricity spot market prices
- Metric: `Spot Price (EUR)`
- Kafka topic: `elspot_topic`

âš ï¸ **Design Note**  
Price and production arrive in **separate Kafka streams** and do **not exist in the same event**.  
They are combined later using **daily aggregation and logical joins**.

---

## ğŸ› ï¸ Technologies Used

```| Layer | Technology |
|-----|-----------|
| Programming | Python 3 |
| Streaming | Apache Kafka |
| Messaging | kafka-python |
| Database | PostgreSQL |
| Containerization | Docker |
| Visualization | Streamlit, Altair |
| API Access | Requests |
| Data Processing | Pandas |```

---

## ğŸ—‚ï¸ Project Structure

```Energi_project/
â”‚
â”œâ”€â”€ s1_test_apis.py # API connectivity test
â”œâ”€â”€ s2_kafka_producer.py # Kafka producer (price + production)
â”œâ”€â”€ s3_kafka_consumer_energi_db.py # Kafka consumer â†’ PostgreSQL
â”œâ”€â”€ s4_streamlit_dashboard.py # Streamlit analytics dashboard
â”‚
â”œâ”€â”€ docker-compose.yml # Kafka, Zookeeper, PostgreSQL
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # Environment variables
â””â”€â”€ README.md # Project documentation```


---

## ğŸ§ª Data Storage Model

All streaming data is stored in a single table:

### `energi_records`

| Column | Description |
|------|------------|
| `timestamp` | Event timestamp |
| `price_area` | Market area (DK1, DK2, DE, etc.) |
| `production_mwh` | Production amount (nullable) |
| `spot_price_eur` | Spot price (nullable) |
| `production_type` | Energy source |
| `source` | Kafka topic origin |
| `payload` | Raw JSON payload |

ğŸ“Œ Nullable fields are **expected** due to asynchronous streams.

---

## ğŸ“Š Streamlit Dashboard Features

### 1ï¸âƒ£ Raw Data Tables
- Latest **Elspot price records**
- Latest **Declaration production records**

### 2ï¸âƒ£ Daily Aggregated Table
- Daily **Total Production (MWh)**
- Daily **Average Spot Price (EUR)**
- Aggregated by **Date and Area**

### 3ï¸âƒ£ Total Production Summary
- Cumulative production per area

### 4ï¸âƒ£ Decision Support Table

| Decision | Logic |
|--------|------|
| BUY | Low price & high production |
| AVOID | High price & low production |
| MONITOR | Mixed market signals |

### 5ï¸âƒ£ Price vs Production Comparison Graph
- Two **line charts** (price vs production)
- Daily aggregated comparison
- Interactive tooltips

---

## ğŸ§  Big Data Design Considerations

- Proper handling of **asynchronous streams**
- No incorrect row-level joins
- Aggregation before analytics
- Robust API error handling
- Modular and scalable architecture

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Start Infrastructure
```bash
docker-compose up -d
2ï¸âƒ£ Start Kafka Producer
python s2_kafka_producer.py

3ï¸âƒ£ Start Kafka Consumer
python s3_kafka_consumer_energi_db.py

4ï¸âƒ£ Launch Streamlit Dashboard
streamlit run s4_streamlit_dashboard.py

ğŸ“ Academic Relevance

This project demonstrates core Big Data concepts:

Streaming data ingestion

Kafka producers and consumers

Data validation and quality

Real-time processing

Aggregation and analytics

Visualization and decision support

ğŸ§¾ Key Learning Outcomes

Designing pipelines with non-overlapping streams

Handling real-world API instability

Separating ingestion from analytics

Building explainable dashboards

Translating raw streams into insights