# âš¡ Energi Data Streaming Project

This project shows how to collect and process **real-time Danish energy data** using:
- **Kafka** for live streaming  
- **PostgreSQL** for storage  
- **Streamlit** for visualization  

You will stream data from [EnergiDataService.dk](https://www.energidataservice.dk/), store it in a database, and display it on a live dashboard.

---

## ğŸ§© What Youâ€™ll Build

EnergiDataService API â†’ Kafka â†’ PostgreSQL â†’ Streamlit Dashboard


### ğŸ’¡ Example Flow:
1. **Producer (Python)** gets data from the Energi API  
2. **Kafka** streams that data in real-time  
3. **Consumer (Python)** receives the data and saves it in PostgreSQL  
4. **Streamlit Dashboard** displays the latest data visually  

---

## âš™ï¸ Tools Used

| Component | Purpose |
|------------|----------|
| **Python** | Main language for producer, consumer, and dashboard |
| **Kafka** | Handles real-time data streaming |
| **Zookeeper** | Required for Kafka to run |
| **PostgreSQL** | Stores the energy data |
| **Docker** | Runs Kafka, Zookeeper, and PostgreSQL easily |
| **Streamlit** | Creates a simple live dashboard |

---

## ğŸ—ï¸ Folder Structure

Energi_project/
â”‚
â”œâ”€â”€ docker-compose.yml # Starts Kafka, Zookeeper, PostgreSQL
â”œâ”€â”€ s1_test_apis.py # Tests EnergiDataService APIs
â”œâ”€â”€ s2_kafka_producer_energi.py # Sends API data to Kafka topics
â”œâ”€â”€ s3_kafka_consumer_energi.py # Reads data from Kafka and saves to PostgreSQL
â”œâ”€â”€ s4_streamlit_dashboard.py # Displays data on Streamlit dashboard
â”œâ”€â”€ create_table_energi.sql # SQL to create table in PostgreSQL
â””â”€â”€ README.md # Project guide


---

## ğŸš€ Step-by-Step Setup

### 1ï¸âƒ£ Start Docker Services
Make sure **Docker Desktop** is running.  
Then open a terminal in your project folder and run:

```bash
docker-compose up -d

You should see 3 running containers:
energi_zookeeper
energi_kafka
energi_postgres

2ï¸âƒ£ Create PostgreSQL Table
docker exec -it energi_postgres psql -U postgres -d energi_data

Then inside PostgreSQL, run:
CREATE TABLE energi_records (
    id SERIAL PRIMARY KEY,
    source VARCHAR(50),
    price_area VARCHAR(10),
    production_type VARCHAR(50),
    co2_per_kwh FLOAT,
    production_mwh FLOAT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
Exit with:
\q
3ï¸âƒ£ Run Kafka Producer
This script gets data from the Energi API and sends it to Kafka.
python s2_kafka_producer.py
4ï¸âƒ£ Run Kafka Consumer
This script reads from Kafka and saves data to PostgreSQL.
python s3_kafka_consumer.py
5ï¸âƒ£ Check Saved Data

Run this command to view whatâ€™s in the database:
docker exec -it energi_postgres psql -U postgres -d energi_data -c "SELECT * FROM energi_records;"
6ï¸âƒ£ Launch Streamlit Dashboard
Finally, launch the Streamlit app:
python -m streamlit run s4_streamlit_dashboard.py
Then open in your browser:
ğŸ‘‰ http://localhost:8501

Youâ€™ll see:

A table of latest energy data

A bar chart of COâ‚‚ emissions by Price Area

A â€œRefreshâ€ button to update the view